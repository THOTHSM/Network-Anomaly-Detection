{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bdbd51d-f947-4a47-aed4-a6a4d3bc4908",
   "metadata": {},
   "source": [
    "### <b> <i> <center>Model Preparation for Flask Deployment :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2295be-8267-4a40-bdda-6427cb8fc620",
   "metadata": {},
   "source": [
    "### Model Preparation for Flask Deployment\n",
    "\n",
    "**Overview:**\n",
    "\n",
    "**1. Data Preprocessing:**\n",
    "   - **Outlier Detection**: Identifies and removes outliers using both Z-Score and IQR methods.\n",
    "   - **Feature Engineering**: Creates new features such as `total_bytes`, `byte_ratio`, and combined error rates while dropping irrelevant columns.\n",
    "   - **Categorical Encoding**: Encodes categorical features using CatBoostEncoder and label encoding.\n",
    "\n",
    "**2. Data Filtering:**\n",
    "   - **Class Filtering**: Removes classes with fewer samples than a specified threshold to ensure sufficient representation.\n",
    "\n",
    "**3. Data Splitting and Resampling:**\n",
    "   - **Train-Test Split**: Divides the data into training and test sets.\n",
    "   - **SMOTE**: Applies Synthetic Minority Over-sampling Technique (SMOTE) to balance the classes in the training set.\n",
    "\n",
    "**4. Model Training and Tuning:**\n",
    "   - **Bagging Classifier**: Utilizes Bagging with a Decision Tree base estimator.\n",
    "   - **Hyperparameter Tuning**: Performs GridSearchCV to find the best hyperparameters for the Bagging Classifier.\n",
    "   - **Model Evaluation**: Evaluates the best model based on cross-validation and test accuracy.\n",
    "\n",
    "**5. Results:**\n",
    "   - **Best Parameters and Accuracy**: Outputs the optimal parameters and accuracy of the Bagging Classifier.\n",
    "\n",
    "This prepared model can now be deployed via a Flask API, enabling real-time predictions and integration into applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a433d131-553f-49d2-ae90-066dcf54e8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Bagging: {'estimator__max_depth': None, 'n_estimators': 100}\n",
      "Best cross-validation accuracy for Bagging: 0.9992033934879475\n",
      "Test accuracy for Bagging: 0.9995124775282611\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import category_encoders as ce\n",
    "import pickle\n",
    "\n",
    "# Loading the data\n",
    "df = pd.read_csv('NAD.csv')\n",
    "\n",
    "# Selecting only numeric columns\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Step 1: Z-Score Method\n",
    "# Calculating Z-Scores for each column\n",
    "z_scores = stats.zscore(numeric_df)\n",
    "z_scores_df = pd.DataFrame(z_scores, columns=numeric_df.columns)\n",
    "z_threshold = 3.5\n",
    "# Identifying outliers based on Z-scores\n",
    "z_outliers = (z_scores_df.abs() > z_threshold)\n",
    "z_outlier_indices = z_outliers[z_outliers.any(axis=1)].index\n",
    "\n",
    "# Step 2: IQR Method\n",
    "# Calculating Q1 (25th percentile) and Q3 (75th percentile)\n",
    "Q1 = numeric_df.quantile(0.25)\n",
    "Q3 = numeric_df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "iqr_threshold = 1.5 \n",
    "iqr_outliers = ((numeric_df < (Q1 - iqr_threshold * IQR)) | (numeric_df > (Q3 + iqr_threshold * IQR)))\n",
    "iqr_outlier_indices = iqr_outliers[iqr_outliers.any(axis=1)].index\n",
    "\n",
    "# Finding common outliers\n",
    "common_outlier_indices = z_outlier_indices.intersection(iqr_outlier_indices)\n",
    "\n",
    "df_cleaned = df.drop(index=common_outlier_indices, errors='ignore')\n",
    "df_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Verifying the size after dropping outliers\n",
    "df = df_cleaned\n",
    "df['total_bytes'] = df['srcbytes'] + df['dstbytes']\n",
    "df['byte_ratio'] = df['srcbytes'] / (df['dstbytes'] + 1)\n",
    "df.drop(columns=['srcbytes', 'dstbytes'], inplace=True)\n",
    "df.drop(columns=['wrongfragment', 'urgent'], inplace=True)\n",
    "df['combined_serror_rerror_rate'] = (df['serrorrate'] + df['rerrorrate']) / 2\n",
    "df['combined_srv_serror_rerror_rate'] = (df['srvserrorrate'] + df['srvrerrorrate']) / 2\n",
    "df['ratio_samesrvrate_diffsrvrate'] = df['samesrvrate'] / (df['diffsrvrate'] + 1e-6)\n",
    "df['service_host_distribution_ratio'] = df['samesrvrate'] / (df['srvdiffhostrate'] + 1e-6)\n",
    "df['combined_dsthostserrorrate_dsthostrerrorrate'] = df['dsthostserrorrate'] + (df['dsthostrerrorrate'] + 1e-6)\n",
    "df['combined_dsthostsrvserrorrate_dsthostsrvrerrorrate'] = df['dsthostsrvserrorrate'] + (df['dsthostsrvrerrorrate'] + 1e-6)\n",
    "df.drop(columns=['serrorrate', 'rerrorrate', 'srvserrorrate', 'srvrerrorrate',\n",
    "                 'samesrvrate', 'diffsrvrate', 'srvdiffhostrate', 'dsthostserrorrate',\n",
    "                 'dsthostrerrorrate', 'dsthostsrvserrorrate', 'dsthostsrvrerrorrate'], inplace=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['attack_encoded'] = label_encoder.fit_transform(df['attack'])\n",
    "df.drop('attack', axis=1, inplace=True)\n",
    "\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Initializing CatBoostEncoder\n",
    "catboost_encoder = ce.CatBoostEncoder(cols=categorical_columns[:3])\n",
    "\n",
    "# Fit and transform the categorical features\n",
    "df_encoded_features = catboost_encoder.fit_transform(df[categorical_columns[:3]], df['attack_encoded'])\n",
    "df[categorical_columns[:3]] = df_encoded_features\n",
    "\n",
    "X = df.drop(columns=['attack_encoded'])\n",
    "y = df['attack_encoded']\n",
    "\n",
    "# Finding classes with fewer samples than the threshold\n",
    "threshold = 6\n",
    "class_counts = df['attack_encoded'].value_counts()\n",
    "classes_to_remove = class_counts[class_counts < threshold].index\n",
    "\n",
    "# Droping rows with these classes\n",
    "df_filtered = df[~df['attack_encoded'].isin(classes_to_remove)]\n",
    "\n",
    "# Separating  features and target\n",
    "X = df_filtered.drop('attack_encoded', axis=1)\n",
    "y = df_filtered['attack_encoded']\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Applying SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "X, y = X_resampled, y_resampled\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# StratifiedKFold\n",
    "stratified_kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)  \n",
    "# Initializing the Decision Tree and Bagging Classifier\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "bagging_clf = BaggingClassifier(estimator=decision_tree, random_state=42) \n",
    "# Hyperparameter tuning for Bagging\n",
    "param_grid_bagging = {\n",
    "    'estimator__max_depth': [None, 1, 2, 4, 6, 8, 10], \n",
    "    'n_estimators': [10, 100]\n",
    "}\n",
    "\n",
    "grid_search_bagging = GridSearchCV(bagging_clf, param_grid_bagging, cv=stratified_kfold, scoring='accuracy')\n",
    "grid_search_bagging.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and cross-validation accuracy for Bagging\n",
    "print(\"Best parameters for Bagging:\", grid_search_bagging.best_params_)\n",
    "print(\"Best cross-validation accuracy for Bagging:\", grid_search_bagging.best_score_)\n",
    "\n",
    "# Testing the best model on the test set\n",
    "best_bagging_model = grid_search_bagging.best_estimator_\n",
    "y_pred_bagging = best_bagging_model.predict(X_test)\n",
    "print(\"Test accuracy for Bagging:\", accuracy_score(y_test, y_pred_bagging))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4f2f91-a5df-4945-9450-e5467b762da8",
   "metadata": {},
   "source": [
    "### <i> <b> Saving the Trained Model and Encoders : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c77330-575f-4482-afc1-d33805385dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Sav9ng the trained model\n",
    "with open('network_anomaly_detection_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_bagging_model, file)\n",
    "# Saving the label encoder\n",
    "with open('label_encoder.pkl', 'wb') as file:\n",
    "    pickle.dump(label_encoder, file)\n",
    "# Saving the CatBoost encoder\n",
    "with open('catboost_encoder.pkl', 'wb') as file:\n",
    "    pickle.dump(catboost_encoder, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ddc56-c3c5-4fa9-9b09-88c0b40a7b0c",
   "metadata": {},
   "source": [
    "In this section, we save the trained model and preprocessing components for future use:\n",
    "\n",
    "- **Model Saving**: The trained Bagging classifier model (`best_bagging_model`) is saved using the `pickle` library. This allows us to easily load and deploy the model without retraining it.\n",
    "\n",
    "- **Label Encoder Saving**: The `LabelEncoder`, which was used to encode the target variable, is also saved. This ensures that we can consistently decode the target labels in future predictions.\n",
    "\n",
    "- **CatBoost Encoder Saving**: The `CatBoostEncoder`, used for encoding categorical features, is saved to ensure that the same encoding is applied during future data preprocessing.\n",
    "\n",
    "This approach facilitates the deployment and reproducibility of the model in various environments.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
